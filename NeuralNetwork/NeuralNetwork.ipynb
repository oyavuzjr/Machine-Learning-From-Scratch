{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "encoder=OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Into Testing And Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to split the data into testing and training sets.  \n",
    "\n",
    "We will work with the famous Iris dataset, loaded from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "data=np.array(iris['data'])\n",
    "target=np.array(iris['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=10)\n",
    "y_train=encoder.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test =(encoder.fit_transform(y_test.reshape(-1,1)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nature of Neural Networks, we must scale the predictors into values between 0 and 1.  \n",
    "We can do this by applying a lambda function to each row for both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_col = lambda c : c/np.max(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.apply_along_axis(scale_col, 0, X_train)\n",
    "X_test =np.apply_along_axis(scale_col, 0, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our activation function is sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Matrix 1\n",
    "W1=np.random.rand(4,3)\n",
    "B1=np.random.rand(3,1)\n",
    "J=np.ones(len(X_train)).reshape(100,1)\n",
    "Z2=(X_train@W1)+(J@B1.T)\n",
    "#Activation Function\n",
    "A2=sigmoid(Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2=np.random.rand(3,3)\n",
    "B2=np.random.rand(3,1)\n",
    "Z3=(A2@W2)+(J@B2.T)\n",
    "y_hat=sigmoid(Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y,yh):\n",
    "    return 0.5*np.sum((y - yh)**2)\n",
    "\n",
    "def sigmoidprime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta3=(-(y_train-y_hat))*sigmoidprime(Z3)\n",
    "delta2=(delta3@W2.T)*sigmoidprime(Z2)\n",
    "djdw2=A2.T@delta3\n",
    "djdb2=np.apply_along_axis(np.sum,0,delta3)\n",
    "djdw1=X_train.T@delta2\n",
    "djdb1=np.apply_along_axis(np.sum,0,delta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Observations\n",
    "n=len(X_train)\n",
    "#Number of Iterations\n",
    "N=10000\n",
    "#Constant Gamma\n",
    "gamma=0.15\n",
    "\n",
    "#Generate Initial Random Weights\n",
    "W1=np.random.rand(4,3)\n",
    "B1=np.random.rand(3,1)\n",
    "W2=np.random.rand(3,3)\n",
    "B2=np.random.rand(3,1)\n",
    "\n",
    "#Start Training!\n",
    "costs=[]\n",
    "for i in range(0,N):\n",
    "    #Calculate the Predicted y values with the current weights\n",
    "    Z2=(X_train@W1)+(J@(B1.T))\n",
    "    A2=sigmoid(Z2)\n",
    "    Z3=(A2@W2)+(J@B2.T)\n",
    "    y_hat=sigmoid(Z3)\n",
    "    costs.append(cost(y_train,y_hat))\n",
    "    #Calculate the Gradient with the current Weights.\n",
    "    delta3=(-(y_train-y_hat))*sigmoidprime(Z3)\n",
    "    delta2=(delta3@W2.T)*sigmoidprime(Z2)\n",
    "    djdw2=A2.T@delta3\n",
    "    djdb2=np.apply_along_axis(np.sum,0,delta3).reshape(3,1)\n",
    "    djdw1=X_train.T@delta2\n",
    "    djdb1=np.apply_along_axis(np.sum,0,delta2).reshape(3,1)\n",
    "    #Take a step in the right direction of the weights' gradient.\n",
    "    W1=W1-djdw1*gamma\n",
    "    W2=W2-djdw2*gamma\n",
    "    B1=B1-djdb1*gamma\n",
    "    B2=B2-djdb2*gamma\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J=np.ones(len(X_test)).reshape(-1,1)\n",
    "Z2=(X_test@W1)+(J@(B1.T))\n",
    "A2=sigmoid(Z2)\n",
    "Z3=(A2@W2)+(J@B2.T)\n",
    "y_hat=sigmoid(Z3)\n",
    "y_hat=np.array((np.round(y_hat,decimals=2)>0.5),dtype=np.int)\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  0,  0],\n",
       "       [ 0, 19,  0],\n",
       "       [ 0,  2, 14]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.argmax(axis=1), y_hat.argmax(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
